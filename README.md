
# ANS Data Pipeline

Este projeto tem como objetivo coletar, transformar, armazenar e disponibilizar dados sobre operadoras de planos de saÃºde registradas na **AgÃªncia Nacional de SaÃºde Suplementar (ANS)**. A soluÃ§Ã£o inclui automaÃ§Ã£o de download de arquivos, extraÃ§Ã£o e limpeza de dados, estruturaÃ§Ã£o em banco de dados e disponibilizaÃ§Ã£o via API.

## ğŸ“‚ Estrutura do Projeto

O projeto estÃ¡ dividido em quatro principais mÃ³dulos:

```
intuitive-care/
â”‚-- web_scraping/          # Coleta de dados da ANS
â”‚-- data_transformation/   # ExtraÃ§Ã£o e limpeza dos dados
â”‚-- database/              # EstruturaÃ§Ã£o e anÃ¡lise dos dados
â”‚-- api/                   # API para acesso aos dados e frontend
â”‚-- intuitive-care-postman-collection.json        # Arquivos da collection do Postman
â”‚-- docker-compose.yml     # ConfiguraÃ§Ã£o do ambiente Docker
â”‚-- README.md              # DocumentaÃ§Ã£o principal
```

---

## 1ï¸âƒ£ Web Scraping - ANSScraper

Este mÃ³dulo baixa automaticamente os arquivos PDF da ANS contendo a atualizaÃ§Ã£o do rol de procedimentos.

### ğŸš€ Tecnologias Utilizadas
- **Python**
- **Requests** - RequisiÃ§Ãµes HTTP
- **BeautifulSoup** - ExtraÃ§Ã£o de links
- **ThreadPoolExecutor** - Download paralelo
- **Zipfile** - CompactaÃ§Ã£o dos arquivos
- **Logging** - Monitoramento

### ğŸ›  Como Executar

1. Instale as dependÃªncias:
   ```sh
   python -m venv venv
   source venv/bin/activate  # Linux/macOS
   venv\Scripts\activate  # Windows
   pip install -r requirements.txt
   ```
2. Execute o script:
   ```sh
   python web_scraping/ans_scraper.py
   ```
   - Os PDFs serÃ£o baixados para `output/`
   - O arquivo `anexos.zip` serÃ¡ gerado

---

## 2ï¸âƒ£ TransformaÃ§Ã£o de Dados

O mÃ³dulo **extrai e limpa** dados tabulares dos PDFs usando `tabula-py` e `pandas`.

### ğŸš€ Tecnologias Utilizadas
- **Python**
- **tabula-py** - ExtraÃ§Ã£o de tabelas de PDFs
- **pandas** - Limpeza e manipulaÃ§Ã£o de dados

### ğŸ›  Como Executar

1. Instale as dependÃªncias:
   ```sh
   pip install pandas tabula-py
   ```
   **Obs:** Certifique-se de ter o Java instalado para o `tabula-py`.

2. Coloque os arquivos PDF na pasta `input/`.

3. Execute o script:
   ```sh
   python data_transformation/extract_table.py
   ```
   - O CSV limpo serÃ¡ salvo em `output/Rol_Procedimentos.csv`
   - Um ZIP contendo os dados serÃ¡ gerado em `output/`
   - O processo serÃ¡ registrado em `transform.log`

---

## 3ï¸âƒ£ Banco de Dados

Os dados extraÃ­dos sÃ£o estruturados em um banco PostgreSQL, permitindo anÃ¡lises sobre as operadoras de saÃºde.

### ğŸš€ Tecnologias Utilizadas
- **PostgreSQL**
- **Docker**
- **SQL para consultas analÃ­ticas**

### ğŸ›  Como Configurar

1. Suba o ambiente Docker:
   ```sh
   docker-compose up -d
   ```
2. Acesse o PgAdmin:
   - URL: [http://localhost:5050](http://localhost:5050)
   - UsuÃ¡rio: `admin@ans.com`
   - Senha: `admin123`
3. Execute os scripts SQL:
   ```sh
   docker exec -it ans_postgres bash
   psql -U ans_admin -d ans_db
   \i /scripts/01_setup.sql;
   \i /scripts/02_import.sql;
   \i /scripts/03_analytics.sql;
   ```

### ğŸ“Š Consultas AnalÃ­ticas

- **Top 10 Operadoras com Maiores Despesas no Ãšltimo Trimestre**
- **Top 10 Operadoras com Maiores Despesas no Ãšltimo Ano**

Os scripts SQL correspondentes podem ser encontrados em `database/scripts/`.

---

## 4ï¸âƒ£ API e Frontend

A API permite acesso aos dados e o frontend fornece uma interface para consultas.

### ğŸš€ Tecnologias Utilizadas
- **FastAPI** - Backend
- **Node.js** - Frontend (React)
- **Docker**

### ğŸ›  Como Executar

#### ğŸ”¹ Backend

1. Instale as dependÃªncias:
   ```sh
   pip install -r api/backend/requirements.txt
   ```
2. Execute o servidor FastAPI:
   ```sh
   uvicorn api.backend.src.main:app --reload --port 8080
   ```
3. Acesse a documentaÃ§Ã£o da API:  
   [http://localhost:8080/docs](http://localhost:8080/docs)

#### ğŸ”¹ Frontend

1. Instale as dependÃªncias:
   ```sh
   cd api/frontend
   npm install
   ```
2. Inicie o frontend:
   ```sh
   npm start
   ```

---

## ğŸ“¬ Testando a API no Postman

Para facilitar a consulta dos dados, foi criada uma **Collection no Postman** chamada **"intuitive-care"**.  

### ğŸ”— Link para a Collection  
VocÃª pode acessar e importar a Collection diretamente pelo link abaixo:  
[Importar Collection no Postman](https://matheusleal-58517.postman.co/workspace/Matheus-Leal's-Workspace~080349ba-08ac-4554-86b9-96bbda2fcfad/collection/43611296-c8959fdb-c807-4344-a158-9ee2de106bca?action=share&source=collection_link&creator=43611296)

### ğŸ“Œ Endpoints DisponÃ­veis

- **Buscar operadoras de saÃºde por nome (com limite de resultados)**  
  **GET** `http://localhost:8080/api/operadoras?q=Itau&limit=10`

  **Exemplo de resposta:**
  ```json
  [
    {
      "id": 1,
      "nome": "Itau Seguros de SaÃºde",
      "registro": "123456",
      "cnpj": "12.345.678/0001-90",
      "tipo": "Administradora de BenefÃ­cios",
      "status": "Ativa"
    },
    ...
  ]
  ```

- **Consultar detalhes de uma operadora especÃ­fica**  
  **GET** `http://localhost:8080/api/operadoras/{id}`

- **Obter estatÃ­sticas sobre operadoras**  
  **GET** `http://localhost:8080/api/estatisticas`

### ğŸ“¥ ImportaÃ§Ã£o Manual  

Caso prefira importar manualmente a Collection, vocÃª pode baixar o arquivo `intuitive-cafe-postman-collection.json` localizado no diretÃ³rio `/postman/` e importÃ¡-lo no Postman.

---

## âœ… ConclusÃ£o

Este projeto integra **Web Scraping, TransformaÃ§Ã£o de Dados, Banco de Dados e API** para fornecer informaÃ§Ãµes estruturadas sobre operadoras de saÃºde da ANS. Ele permite:

âœ”ï¸ **Coleta automÃ¡tica** de documentos oficiais  
âœ”ï¸ **ExtraÃ§Ã£o e limpeza** de dados tabulares  
âœ”ï¸ **Armazenamento** em um banco relacional otimizado  
âœ”ï¸ **Acesso via API** para anÃ¡lise e consulta  
âœ”ï¸ **Testes e visualizaÃ§Ã£o de dados via Postman**  

Caso tenha dÃºvidas ou precise modificar alguma funcionalidade, consulte a documentaÃ§Ã£o de cada mÃ³dulo ou me chame pelo linkedin.
